{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0cd7013-bf38-40ff-9bb4-023dde56a7a6",
   "metadata": {},
   "source": [
    "# Option I: Predicting Onset of Rainy Season Using Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231df225-b8b6-41ee-91a0-19d33caf6f82",
   "metadata": {},
   "source": [
    "This notebook demonstrates the use of ML models to predict the onset of the rainy season based on daily rainfall data. We will explore feature engineering, supervised learning approaches, and model evaluation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556d79c8-ea93-43e2-b2aa-2fd685ad0790",
   "metadata": {},
   "source": [
    "## Onset definition \n",
    "The definition of the onset is looking at a significantly wet event (e.g. 20mm in 3 days) that is not followed by a dry spell (e.g. 7-day dry spell in the following 21 days). The actual date is the first wet day of the wet event. The onset date is computed on the fly for each year according to the definition and is expressed in days since an early start date (e.g., Feb 1st). The onset date is searched from that early start date and for a certain number of following days (e.g. 60 days). The early start date serves as a reference and should be picked so that it is ahead of the expected onset date.\n",
    "\n",
    "- Steps to compute onset\n",
    "  - Initialize Parameters: Define the early start date, the number of days to check for the onset, and the criteria for the onset (e.g., 20 mm in 3 days without a 7-day dry spell in the following 21 days).\n",
    "  - Loop Through Each Year: For each year, begin the search for the onset from the early start date and check for a wet event.\n",
    "  - Check for Wet Event: The onset is identified when thereâ€™s a cumulative rainfall of 20 mm over 3 consecutive days.\n",
    "  - Check for Dry Spell: After detecting a wet event, ensure that there is no 7-day dry spell in the subsequent 21 days.- Store Onset Date: If both conditions are met, record the first day of the wet event as the onset date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "639a049a-0fa6-42d4-a15b-508d0c6b6fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9812b43-df11-464f-9740-25945f2bf325",
   "metadata": {},
   "source": [
    "## Loading and preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da7ba0c7-592e-439d-add1-5a0e4bfee9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the daily rainfall data\n",
    "rainfall_df = pd.read_csv('/Users/jemal/Desktop/Bootcamp_UK/Group_Project/EDACaP040706_daily.csv')\n",
    "rainfall_df['date'] = pd.to_datetime(rainfall_df[['year', 'month', 'day']])\n",
    "rainfall_df['year'] = rainfall_df['date'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beab9f46-fcfc-4ff7-bc71-5f83c4f91888",
   "metadata": {},
   "source": [
    "#### Onset detection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5b217479-6691-491a-8733-60b6c4accd41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>onset_date</th>\n",
       "      <th>days_since_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1981</td>\n",
       "      <td>1981-04-18</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1982</td>\n",
       "      <td>1982-05-06</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1983</td>\n",
       "      <td>1983-04-03</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1984</td>\n",
       "      <td>1984-05-16</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1985</td>\n",
       "      <td>1985-04-14</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1986</td>\n",
       "      <td>1986-04-25</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1987</td>\n",
       "      <td>1987-04-04</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1988</td>\n",
       "      <td>1988-04-16</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1989</td>\n",
       "      <td>1989-04-07</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1990</td>\n",
       "      <td>1990-04-02</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1991</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1992</td>\n",
       "      <td>1992-04-27</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1993</td>\n",
       "      <td>1993-04-03</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1994</td>\n",
       "      <td>1994-04-24</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1995</td>\n",
       "      <td>1995-04-05</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1996</td>\n",
       "      <td>1996-04-16</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1997</td>\n",
       "      <td>1997-04-06</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1998</td>\n",
       "      <td>1998-04-21</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1999</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2000</td>\n",
       "      <td>2000-04-14</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2001</td>\n",
       "      <td>2001-05-01</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2002</td>\n",
       "      <td>2002-04-10</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2003</td>\n",
       "      <td>2003-04-10</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2004</td>\n",
       "      <td>2004-04-03</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2005</td>\n",
       "      <td>2005-04-23</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2006</td>\n",
       "      <td>2006-04-01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2007</td>\n",
       "      <td>2007-04-11</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2008</td>\n",
       "      <td>2008-04-07</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2009</td>\n",
       "      <td>2009-04-01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2010</td>\n",
       "      <td>2010-04-11</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2011</td>\n",
       "      <td>2011-04-25</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2012</td>\n",
       "      <td>2012-04-01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2013</td>\n",
       "      <td>2013-04-30</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2014</td>\n",
       "      <td>2014-05-24</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2015</td>\n",
       "      <td>2015-04-15</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2016</td>\n",
       "      <td>2016-04-03</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2017</td>\n",
       "      <td>2017-05-13</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2018</td>\n",
       "      <td>2018-04-16</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2019</td>\n",
       "      <td>2019-04-02</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2020</td>\n",
       "      <td>2020-04-16</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year onset_date  days_since_start\n",
       "0   1981 1981-04-18              17.0\n",
       "1   1982 1982-05-06              35.0\n",
       "2   1983 1983-04-03               2.0\n",
       "3   1984 1984-05-16              45.0\n",
       "4   1985 1985-04-14              13.0\n",
       "5   1986 1986-04-25              24.0\n",
       "6   1987 1987-04-04               3.0\n",
       "7   1988 1988-04-16              15.0\n",
       "8   1989 1989-04-07               6.0\n",
       "9   1990 1990-04-02               1.0\n",
       "10  1991        NaT               NaN\n",
       "11  1992 1992-04-27              26.0\n",
       "12  1993 1993-04-03               2.0\n",
       "13  1994 1994-04-24              23.0\n",
       "14  1995 1995-04-05               4.0\n",
       "15  1996 1996-04-16              15.0\n",
       "16  1997 1997-04-06               5.0\n",
       "17  1998 1998-04-21              20.0\n",
       "18  1999        NaT               NaN\n",
       "19  2000 2000-04-14              13.0\n",
       "20  2001 2001-05-01              30.0\n",
       "21  2002 2002-04-10               9.0\n",
       "22  2003 2003-04-10               9.0\n",
       "23  2004 2004-04-03               2.0\n",
       "24  2005 2005-04-23              22.0\n",
       "25  2006 2006-04-01               0.0\n",
       "26  2007 2007-04-11              10.0\n",
       "27  2008 2008-04-07               6.0\n",
       "28  2009 2009-04-01               0.0\n",
       "29  2010 2010-04-11              10.0\n",
       "30  2011 2011-04-25              24.0\n",
       "31  2012 2012-04-01               0.0\n",
       "32  2013 2013-04-30              29.0\n",
       "33  2014 2014-05-24              53.0\n",
       "34  2015 2015-04-15              14.0\n",
       "35  2016 2016-04-03               2.0\n",
       "36  2017 2017-05-13              42.0\n",
       "37  2018 2018-04-16              15.0\n",
       "38  2019 2019-04-02               1.0\n",
       "39  2020 2020-04-16              15.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def detect_onset(df, early_start, max_search_days, wet_event_threshold=20, dry_spell_days=5, dry_spell_window=21):\n",
    "    \"\"\"\n",
    "    Detects the onset date based on the definition:\n",
    "    - Cumulative rainfall of 20mm over 3 consecutive days (wet event).\n",
    "    - No 7-day dry spell within the following 21 days.\n",
    "    \"\"\"\n",
    "    onset_date = None\n",
    "    # Subset the data to the search period (early start date + 60 days)\n",
    "    df = df[(df['date'] >= early_start) & (df['date'] < early_start + pd.Timedelta(days=max_search_days))].reset_index(drop=True)\n",
    "    \n",
    "    for i in range(len(df) - 2):  # Loop through rows to check for wet events\n",
    "        # Check if the cumulative rainfall over 3 days exceeds the threshold\n",
    "        wet_event = df.loc[i:i+2, 'prec'].sum() >= wet_event_threshold\n",
    "        \n",
    "        if wet_event:\n",
    "            # Now check if there's no 7-day dry spell in the next 21 days\n",
    "            future_rainfall = df.loc[i+3:i+3+dry_spell_window, 'prec']\n",
    "            dry_spell = (future_rainfall.rolling(dry_spell_days).sum() == 2).any()\n",
    "            \n",
    "            if not dry_spell:  # If no dry spell, we've found the onset\n",
    "                onset_date = df.loc[i, 'date']  # The first wet day\n",
    "                break\n",
    "    \n",
    "    return onset_date\n",
    "\n",
    "\n",
    "# Detect onset for each year\n",
    "# Parameters\n",
    "early_start = pd.Timestamp(\"1981-04-01\")  # Early start date, e.g., February 1st\n",
    "max_search_days = 60  # Search for onset within 60 days from early start\n",
    "\n",
    "# Initialize a list to store onset dates for each year\n",
    "onset_dates = []\n",
    "\n",
    "# Group by year and detect onset for each year\n",
    "for year, group in rainfall_df.groupby('year'):\n",
    "    early_start_year = pd.Timestamp(f'{year}-04-01')  # Adjust for each year\n",
    "    onset = detect_onset(group, early_start_year, max_search_days)\n",
    "    onset_days_since_start = (onset - early_start_year).days if onset else None\n",
    "    \n",
    "    onset_dates.append({'year': year, 'onset_date': onset, 'days_since_start': onset_days_since_start})\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "onset_df = pd.DataFrame(onset_dates)\n",
    "\n",
    "# Display onset results\n",
    "#import ace_tools as tools; tools.display_dataframe_to_user(name=\"Onset Dates by Year\", dataframe=onset_df)\n",
    "onset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2491560-0726-411e-814b-6e7becce30a7",
   "metadata": {},
   "source": [
    "#### Detect onset for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5f58daa-5009-4d7e-9342-9a79464cddf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>onset_date</th>\n",
       "      <th>days_since_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1981</td>\n",
       "      <td>1981-04-18</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1982</td>\n",
       "      <td>1982-05-06</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1983</td>\n",
       "      <td>1983-04-03</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1984</td>\n",
       "      <td>1984-05-16</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1985</td>\n",
       "      <td>1985-04-14</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1986</td>\n",
       "      <td>1986-04-25</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1987</td>\n",
       "      <td>1987-04-04</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1988</td>\n",
       "      <td>1988-04-16</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1989</td>\n",
       "      <td>1989-04-07</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1990</td>\n",
       "      <td>1990-04-02</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1991</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1992</td>\n",
       "      <td>1992-04-27</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1993</td>\n",
       "      <td>1993-04-03</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1994</td>\n",
       "      <td>1994-04-24</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1995</td>\n",
       "      <td>1995-04-05</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1996</td>\n",
       "      <td>1996-04-16</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1997</td>\n",
       "      <td>1997-04-06</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1998</td>\n",
       "      <td>1998-04-21</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1999</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2000</td>\n",
       "      <td>2000-04-14</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2001</td>\n",
       "      <td>2001-05-01</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2002</td>\n",
       "      <td>2002-04-10</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2003</td>\n",
       "      <td>2003-04-10</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2004</td>\n",
       "      <td>2004-04-03</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2005</td>\n",
       "      <td>2005-04-23</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2006</td>\n",
       "      <td>2006-04-01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2007</td>\n",
       "      <td>2007-04-11</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2008</td>\n",
       "      <td>2008-04-07</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2009</td>\n",
       "      <td>2009-04-01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2010</td>\n",
       "      <td>2010-04-11</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2011</td>\n",
       "      <td>2011-04-25</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2012</td>\n",
       "      <td>2012-04-01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2013</td>\n",
       "      <td>2013-04-30</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2014</td>\n",
       "      <td>2014-05-24</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2015</td>\n",
       "      <td>2015-04-15</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2016</td>\n",
       "      <td>2016-04-03</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2017</td>\n",
       "      <td>2017-05-13</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2018</td>\n",
       "      <td>2018-04-16</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2019</td>\n",
       "      <td>2019-04-02</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2020</td>\n",
       "      <td>2020-04-16</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year onset_date  days_since_start\n",
       "0   1981 1981-04-18              17.0\n",
       "1   1982 1982-05-06              35.0\n",
       "2   1983 1983-04-03               2.0\n",
       "3   1984 1984-05-16              45.0\n",
       "4   1985 1985-04-14              13.0\n",
       "5   1986 1986-04-25              24.0\n",
       "6   1987 1987-04-04               3.0\n",
       "7   1988 1988-04-16              15.0\n",
       "8   1989 1989-04-07               6.0\n",
       "9   1990 1990-04-02               1.0\n",
       "10  1991        NaT               NaN\n",
       "11  1992 1992-04-27              26.0\n",
       "12  1993 1993-04-03               2.0\n",
       "13  1994 1994-04-24              23.0\n",
       "14  1995 1995-04-05               4.0\n",
       "15  1996 1996-04-16              15.0\n",
       "16  1997 1997-04-06               5.0\n",
       "17  1998 1998-04-21              20.0\n",
       "18  1999        NaT               NaN\n",
       "19  2000 2000-04-14              13.0\n",
       "20  2001 2001-05-01              30.0\n",
       "21  2002 2002-04-10               9.0\n",
       "22  2003 2003-04-10               9.0\n",
       "23  2004 2004-04-03               2.0\n",
       "24  2005 2005-04-23              22.0\n",
       "25  2006 2006-04-01               0.0\n",
       "26  2007 2007-04-11              10.0\n",
       "27  2008 2008-04-07               6.0\n",
       "28  2009 2009-04-01               0.0\n",
       "29  2010 2010-04-11              10.0\n",
       "30  2011 2011-04-25              24.0\n",
       "31  2012 2012-04-01               0.0\n",
       "32  2013 2013-04-30              29.0\n",
       "33  2014 2014-05-24              53.0\n",
       "34  2015 2015-04-15              14.0\n",
       "35  2016 2016-04-03               2.0\n",
       "36  2017 2017-05-13              42.0\n",
       "37  2018 2018-04-16              15.0\n",
       "38  2019 2019-04-02               1.0\n",
       "39  2020 2020-04-16              15.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "early_start = pd.Timestamp(\"1981-04-01\")  # Early start date, e.g., February 1st\n",
    "max_search_days = 60  # Search for onset within 60 days from early start\n",
    "\n",
    "# Initialize a list to store onset dates for each year\n",
    "onset_dates = []\n",
    "\n",
    "# Group by year and detect onset for each year\n",
    "for year, group in rainfall_df.groupby('year'):\n",
    "    early_start_year = pd.Timestamp(f'{year}-04-01')  # Adjust for each year\n",
    "    onset = detect_onset(group, early_start_year, max_search_days)\n",
    "    onset_days_since_start = (onset - early_start_year).days if onset else None\n",
    "    \n",
    "    onset_dates.append({'year': year, 'onset_date': onset, 'days_since_start': onset_days_since_start})\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "onset_df = pd.DataFrame(onset_dates)\n",
    "\n",
    "# Display onset results\n",
    "#import ace_tools as tools; tools.display_dataframe_to_user(name=\"Onset Dates by Year\", dataframe=onset_df)\n",
    "onset_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a223954-7f86-4b41-a455-b57647092652",
   "metadata": {},
   "source": [
    "# Option II: ML approach \n",
    "To implement an ML and AI-based onset of rainfall detection based on the criteria you provided, we can approach the problem as a classification task. The idea is to use historical rainfall data and onset definitions to train a machine-learning model that can predict whether a given date marks the onset of the rainy season.\n",
    "\n",
    "Hereâ€™s a structured approach to implement the machine learning-based onset detection:\n",
    "\n",
    "- Feature engineering: we need to extract meaningful features from the rainfall data, including:\n",
    "    - Cumulative rainfall over specific windows (e.g., 3 days).\n",
    "    - Rolling sums and averages.\n",
    "    - Number of consecutive wet/dry days.\n",
    "    - Rainfall variability\n",
    "- Labeling data: we need to label the data points as 1 for onset days (using your provided criteria) and 0 otherwise for supervised learning.\n",
    "- Model selection: we'll use a classifier like Random Forest, Gradient Boosting, or even LSTM (if we want to leverage time-series forecasting, we can use the previous script shared by STM).\n",
    "- Training and evaluation: we split our data into training and test sets, train the model on the historical labeled data, and evaluate its performance.\n",
    "\n",
    "### Feature engineering\n",
    "First We need to create features that help predict whether a given day is the onset date. In our case, our features might include:\n",
    "- Cumulative rainfall over the last 3 days.\n",
    "- Rolling sum of rainfall over the next 21 days.\n",
    "- Whether a dry spell occurred in the following 21 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76d8fe95-1cf7-4dbb-9aed-c012a80a0f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>t_max</th>\n",
       "      <th>t_min</th>\n",
       "      <th>prec</th>\n",
       "      <th>sol_rad</th>\n",
       "      <th>date</th>\n",
       "      <th>cumulative_rain_3days</th>\n",
       "      <th>rolling_sum_21days</th>\n",
       "      <th>dry_spell_7days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1981</td>\n",
       "      <td>25.36</td>\n",
       "      <td>10.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.10</td>\n",
       "      <td>1981-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1981</td>\n",
       "      <td>25.67</td>\n",
       "      <td>10.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.34</td>\n",
       "      <td>1981-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1981</td>\n",
       "      <td>26.64</td>\n",
       "      <td>11.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.72</td>\n",
       "      <td>1981-01-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1981</td>\n",
       "      <td>25.56</td>\n",
       "      <td>10.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.88</td>\n",
       "      <td>1981-01-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1981</td>\n",
       "      <td>25.81</td>\n",
       "      <td>10.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.40</td>\n",
       "      <td>1981-01-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   day  month  year  t_max  t_min  prec  sol_rad       date  \\\n",
       "0    1      1  1981  25.36  10.59   0.0    23.10 1981-01-01   \n",
       "1    2      1  1981  25.67  10.83   0.0    21.34 1981-01-02   \n",
       "2    3      1  1981  26.64  11.16   0.0    22.72 1981-01-03   \n",
       "3    4      1  1981  25.56  10.59   0.0    21.88 1981-01-04   \n",
       "4    5      1  1981  25.81  10.06   0.0    23.40 1981-01-05   \n",
       "\n",
       "   cumulative_rain_3days  rolling_sum_21days  dry_spell_7days  \n",
       "0                    NaN                 NaN            False  \n",
       "1                    NaN                 NaN            False  \n",
       "2                    0.0                 NaN            False  \n",
       "3                    0.0                 NaN            False  \n",
       "4                    0.0                 NaN            False  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add cumulative rainfall and rolling sums\n",
    "rainfall_df['cumulative_rain_3days'] = rainfall_df['prec'].rolling(3).sum()\n",
    "rainfall_df['rolling_sum_21days'] = rainfall_df['prec'].rolling(21).sum()\n",
    "\n",
    "# Define a dry spell as 7 consecutive dry days (precipitation = 0)\n",
    "rainfall_df['dry_spell_7days'] = rainfall_df['prec'].rolling(7).sum() == 0\n",
    "rainfall_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ce7c56-6e60-4879-845a-c86472354635",
   "metadata": {},
   "source": [
    "### Labeling the onset data\n",
    "Weâ€™ll label the dataset based our onset criteria. For each year, we'll label the first day of the wet event (if found) as 1 and all other days as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3a1b34b-8dee-4f3b-be35-363c1264e032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "onset\n",
       "0.0    60\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize labels for onset detection\n",
    "rainfall_df['onset'] = 0\n",
    "\n",
    "# Function to label the onset days\n",
    "def label_onset(df, early_start, max_search_days, wet_event_threshold=20, dry_spell_days=7, dry_spell_window=21):\n",
    "    onset_date = None\n",
    "    # Subset the data to the search period (early start date + 60 days)\n",
    "    df = df[(df['date'] >= early_start) & (df['date'] < early_start + pd.Timedelta(days=max_search_days))].reset_index(drop=True)\n",
    "    \n",
    "    for i in range(len(df) - 2):  # Loop through rows to check for wet events\n",
    "        wet_event = df.loc[i:i+2, 'prec'].sum() >= wet_event_threshold\n",
    "        if wet_event:\n",
    "            future_rainfall = df.loc[i+3:i+3+dry_spell_window, 'prec']\n",
    "            dry_spell = (future_rainfall.rolling(dry_spell_days).sum() == 0).any()\n",
    "            if not dry_spell:\n",
    "                onset_date = df.loc[i, 'date']\n",
    "                df.at[i, 'onset'] = 1  # Mark this date as the onset\n",
    "                break\n",
    "    return df\n",
    "\n",
    "# Label onset dates for each year\n",
    "for year, group in rainfall_df.groupby('year'):\n",
    "    early_start_year = pd.Timestamp(f'{year}-04-01')  # Adjust for each year\n",
    "    rainfall_df.loc[group.index, 'onset'] = label_onset(group, early_start_year, max_search_days=60)['onset']\n",
    "\n",
    "rainfall_df['onset'].value_counts() # Check the distribution of labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f345cfc5-6496-472e-a075-fc584cca0583",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "We will use a Random Forest Classifier for this classification problem. Random Forests are good at handling tabular data and can deal with non-linear patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b983d10d-fa68-400e-98e3-93d84428e03a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Train a Random Forest model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Predict on the test set\u001b[39;00m\n\u001b[1;32m     18\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CAM_Bootcamp/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CAM_Bootcamp/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:363\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 363\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# missing values.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator)(criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CAM_Bootcamp/lib/python3.12/site-packages/sklearn/base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CAM_Bootcamp/lib/python3.12/site-packages/sklearn/utils/validation.py:1318\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1299\u001b[0m     )\n\u001b[1;32m   1301\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1302\u001b[0m     X,\n\u001b[1;32m   1303\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1316\u001b[0m )\n\u001b[0;32m-> 1318\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43m_check_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_numeric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1320\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CAM_Bootcamp/lib/python3.12/site-packages/sklearn/utils/validation.py:1328\u001b[0m, in \u001b[0;36m_check_y\u001b[0;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Isolated part of check_X_y dedicated to y validation\"\"\"\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multi_output:\n\u001b[0;32m-> 1328\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CAM_Bootcamp/lib/python3.12/site-packages/sklearn/utils/validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1061\u001b[0m     )\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m-> 1064\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CAM_Bootcamp/lib/python3.12/site-packages/sklearn/utils/validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CAM_Bootcamp/lib/python3.12/site-packages/sklearn/utils/validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m     )\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define the features (excluding date and year) and target (onset)\n",
    "features = ['cumulative_rain_3days', 'rolling_sum_21days', 'dry_spell_7days']\n",
    "X = rainfall_df[features]\n",
    "y = rainfall_df['onset']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ca6c5f-d892-4e92-9846-a38a53dfa79d",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "If there is limited number of onset days in the dataset, what  we can do is \n",
    " - if onset days are rare, we may need to employ techniques such as oversampling or undersampling to balance the classes\n",
    " - since the ROC-AUC requires both classes to be present, we can evaluate the model using metrics like accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d77416eb-7011-474f-9e66-9058fd46c7f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Fine-tune the Random Forest model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Predict on the test set\u001b[39;00m\n\u001b[1;32m      9\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CAM_Bootcamp/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CAM_Bootcamp/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:363\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 363\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# missing values.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator)(criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CAM_Bootcamp/lib/python3.12/site-packages/sklearn/base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CAM_Bootcamp/lib/python3.12/site-packages/sklearn/utils/validation.py:1318\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1299\u001b[0m     )\n\u001b[1;32m   1301\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1302\u001b[0m     X,\n\u001b[1;32m   1303\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1316\u001b[0m )\n\u001b[0;32m-> 1318\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43m_check_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_numeric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1320\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CAM_Bootcamp/lib/python3.12/site-packages/sklearn/utils/validation.py:1328\u001b[0m, in \u001b[0;36m_check_y\u001b[0;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Isolated part of check_X_y dedicated to y validation\"\"\"\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multi_output:\n\u001b[0;32m-> 1328\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CAM_Bootcamp/lib/python3.12/site-packages/sklearn/utils/validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1061\u001b[0m     )\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m-> 1064\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CAM_Bootcamp/lib/python3.12/site-packages/sklearn/utils/validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CAM_Bootcamp/lib/python3.12/site-packages/sklearn/utils/validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m     )\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "# Removing stratification and performing a regular train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fine-tune the Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf.predict(X_test)\n",
    "y_proba = rf.predict_proba(X_test)[:, 1]  # For ROC-AUC\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report_str = classification_report(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Display results\n",
    "results = {\n",
    "    \"Accuracy\": accuracy,\n",
    "    \"Classification Report\": classification_report_str,\n",
    "    \"ROC-AUC\": roc_auc\n",
    "}\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ab7495-47af-4952-b048-9906ba7c218f",
   "metadata": {},
   "source": [
    "After balancing the dataset using oversampling of the minority class (onset days), the model achieved the following performance metrics\n",
    "- Accuracy: 99.98%\n",
    "- Precision, Recall, and F1-Score for both classes (0 for non-onset and 1 for onset) are all close to 1.0, indicating the model is performing very well on the balanced dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505dc366-2528-4704-b466-e2cdaf0b34dd",
   "metadata": {},
   "source": [
    "# Option II - LSTM (Long Short-Term Memory) \n",
    "To implement an LSTM model for time-series forecasting, we can treat the rainfall data as a sequential problem. LSTM models are well-suited for time-series data because they can capture long-term dependencies and patterns in sequences.\n",
    "\n",
    "Here's the steps that we implemented LSTM for onset detection:\n",
    "- We transform the data into sequences where each sequence includes a certain number of timesteps (e.g., 30 days) leading up to a target value (whether the current day is an onset or not).\n",
    "- The LSTM model requires 3D input, with the shape (samples, timesteps, features).\n",
    "  - We build a simple LSTM model to classify whether a given sequence of days leads to the onset of the rainy season.\n",
    "  - The input is the rainfall data sequences, and the output will be a binary classification of onset or non-onset.\n",
    "  - We split the data into training and test sets, train the LSTM model, and evaluate it using metrics like accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e38585-5d75-418b-af35-ef0ed2226c60",
   "metadata": {},
   "source": [
    "#### Data preparation for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2400a5f2-d521-404c-bb4f-6ae0fe358d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5642673-7f86-43b2-963f-f18b4fc69756",
   "metadata": {},
   "source": [
    " We start by creating features like cumulative rainfall and checking for dry spells, then label the data based on the onset detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0299f0c1-3222-4da8-a00a-1788475a9134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'path_to_your_data/EDACaP040706_daily.csv'  # Update with your file path\n",
    "rainfall_df = pd.read_csv(file_path)\n",
    "\n",
    "# Combine day, month, and year into a single date column\n",
    "rainfall_df['date'] = pd.to_datetime(rainfall_df[['year', 'month', 'day']])\n",
    "\n",
    "# Keep only relevant columns\n",
    "rainfall_df = rainfall_df[['date', 'year', 'prec']]\n",
    "\n",
    "# Feature Engineering: Add cumulative rainfall and rolling sums\n",
    "rainfall_df['cumulative_rain_3days'] = rainfall_df['prec'].rolling(3).sum()\n",
    "rainfall_df['rolling_sum_21days'] = rainfall_df['prec'].rolling(21).sum()\n",
    "rainfall_df['dry_spell_7days'] = rainfall_df['prec'].rolling(7).sum() == 0\n",
    "\n",
    "# Fill NaN values (introduced by rolling)\n",
    "rainfall_df.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553c5733-69be-436e-9648-f00211d00529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect the onset of the rainy season based on the criteria\n",
    "def detect_onset(df, early_start, max_search_days, wet_event_threshold=20, dry_spell_days=7, dry_spell_window=21):\n",
    "    onset_date = None\n",
    "    df = df[(df['date'] >= early_start) & (df['date'] < early_start + pd.Timedelta(days=max_search_days))].reset_index(drop=True)\n",
    "    \n",
    "    for i in range(len(df) - 2):\n",
    "        wet_event = df.loc[i:i+2, 'prec'].sum() >= wet_event_threshold\n",
    "        \n",
    "        if wet_event:\n",
    "            future_rainfall = df.loc[i+3:i+3+dry_spell_window, 'prec']\n",
    "            dry_spell = (future_rainfall.rolling(dry_spell_days).sum() == 0).any()\n",
    "            \n",
    "            if not dry_spell:\n",
    "                onset_date = df.loc[i, 'date']\n",
    "                df.at[i, 'onset'] = 1  # Mark this day as the onset\n",
    "                break\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Label onset dates for each year\n",
    "rainfall_df['onset'] = 0  # Initialize labels (0 = no onset)\n",
    "for year, group in rainfall_df.groupby('year'):\n",
    "    early_start_year = pd.Timestamp(f'{year}-02-01')\n",
    "    rainfall_df.loc[group.index, 'onset'] = detect_onset(group, early_start_year, max_search_days=60)['onset']\n",
    "\n",
    "# Display labeled data\n",
    "rainfall_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dae2bf6-882b-4014-8df9-ef7b2fe6b9aa",
   "metadata": {},
   "source": [
    "We will label the dataset for each year based on the onset detection criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb76f02b-69b8-4d6d-b81c-a93322c29428",
   "metadata": {},
   "source": [
    "####  Prepare data for LSTM model\n",
    "We now prepare the data for the LSTM model by creating sequences of historical data points, including the onset labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f71ec40-c535-46ba-a8d7-c104e7215633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Scale the data for LSTM input\n",
    "scaler = MinMaxScaler()\n",
    "rainfall_df_scaled = scaler.fit_transform(rainfall_df[['prec', 'cumulative_rain_3days', 'rolling_sum_21days', 'dry_spell_7days']])\n",
    "\n",
    "# Define sequence length (e.g., 30 days)\n",
    "sequence_length = 30\n",
    "\n",
    "# Function to create sequences\n",
    "def create_sequences(data, labels, sequence_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i + sequence_length])\n",
    "        y.append(labels[i + sequence_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences\n",
    "X_lstm, y_lstm = create_sequences(rainfall_df_scaled, rainfall_df['onset'].values, sequence_length)\n",
    "\n",
    "# Train-test split\n",
    "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(X_lstm, y_lstm, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the training and test sets\n",
    "X_train_lstm.shape, X_test_lstm.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaf2773-7a62-4ca2-a5e4-793445d29871",
   "metadata": {},
   "source": [
    "#### Train the LSTM Model\n",
    "We will now train the LSTM model to predict the onset of the rainy season.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c4d5a0-48d4-4eae-bdb6-d5e9b10d9b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(units=100, activation='relu', return_sequences=True, input_shape=(sequence_length, X_train_lstm.shape[2]))))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(units=50, activation='relu', return_sequences=False)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Setup callbacks for early stopping and learning rate scheduler\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.00001)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_lstm, y_train_lstm, epochs=50, batch_size=64, validation_data=(X_test_lstm, y_test_lstm),\n",
    "                    callbacks=[early_stopping, lr_scheduler])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c99bd6-f975-41f6-b81e-9d676c957487",
   "metadata": {},
   "source": [
    "##### Evaluate the LSTM Model\n",
    "Finally, we will evaluate the trained LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820d1711-86eb-416a-b082-075860f6acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_lstm = (model.predict(X_test_lstm) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_lstm = accuracy_score(y_test_lstm, y_pred_lstm)\n",
    "precision_lstm = precision_score(y_test_lstm, y_pred_lstm)\n",
    "recall_lstm = recall_score(y_test_lstm, y_pred_lstm)\n",
    "f1_lstm = f1_score(y_test_lstm, y_pred_lstm)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Accuracy: {accuracy_lstm}\")\n",
    "print(f\"Precision: {precision_lstm}\")\n",
    "print(f\"Recall: {recall_lstm}\")\n",
    "print(f\"F1-Score: {f1_lstm}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8026bf-51f7-4c28-ae17-0bca469c3c31",
   "metadata": {},
   "source": [
    "##### Visualize Training Performance\n",
    "You can visualize the loss curves during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd9a3c4-3b38-47bd-bce5-fdf14383e13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1178e2-1c14-43b3-b636-7c030521389c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61347c3-3291-4d0a-b6c8-ca810eb20afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sequence length (number of timesteps in each input sequence)\n",
    "sequence_length = 30\n",
    "\n",
    "# Scaling the rainfall data\n",
    "scaler = MinMaxScaler()\n",
    "rainfall_df_scaled = scaler.fit_transform(rainfall_df[['prec', 'cumulative_rain_3days', 'rolling_sum_21days', 'dry_spell_7days']])\n",
    "\n",
    "# Create sequences for the LSTM model\n",
    "def create_sequences(data, labels, sequence_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i + sequence_length])\n",
    "        y.append(labels[i + sequence_length])  # The label corresponds to the last day in the sequence\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Prepare sequences\n",
    "X_lstm, y_lstm = create_sequences(rainfall_df_scaled, rainfall_df['onset'].values, sequence_length)\n",
    "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(X_lstm, y_lstm, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb793f72-08c7-47d0-9c87-80fbfed32bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, activation='relu', return_sequences=True, input_shape=(sequence_length, X_train_lstm.shape[2])))\n",
    "model.add(Dropout(0.2))  # Dropout to prevent overfitting\n",
    "model.add(LSTM(units=50, activation='relu', return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=1, activation='sigmoid'))  # Output layer for binary classification (onset or non-onset)\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train_lstm, y_train_lstm, epochs=10, batch_size=32, validation_data=(X_test_lstm, y_test_lstm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a809624-a592-44ef-b7c0-16e42a48c833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred_lstm = (model.predict(X_test_lstm) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_lstm = accuracy_score(y_test_lstm, y_pred_lstm)\n",
    "precision_lstm = precision_score(y_test_lstm, y_pred_lstm)\n",
    "recall_lstm = recall_score(y_test_lstm, y_pred_lstm)\n",
    "f1_lstm = f1_score(y_test_lstm, y_pred_lstm)\n",
    "\n",
    "# Display the results\n",
    "lstm_results = {\n",
    "    \"Accuracy\": accuracy_lstm,\n",
    "    \"Precision\": precision_lstm,\n",
    "    \"Recall\": recall_lstm,\n",
    "    \"F1-Score\": f1_lstm\n",
    "}\n",
    "\n",
    "lstm_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af83d776-e95d-4e29-a2bf-3b7283a6f87d",
   "metadata": {},
   "source": [
    "##### Optimize the LSTM model\n",
    "We can definitely optimize the LSTM model further to improve performance. There are several techniques to fine-tune and optimize LSTM models for better accuracy, precision, recall, and other metrics. Hereâ€™s how you can go about it:\n",
    "\n",
    "- Hyperparameter Tuning\n",
    "    - Number of Units in LSTM Layers: You can experiment with the number of units (neurons) in each LSTM layer (e.g., 50, 100, 200). More units can increase the modelâ€™s capacity to learn, but too many can lead to overfitting.\n",
    "    - Number of LSTM Layers: You can stack multiple LSTM layers to increase the depth of the model, but deeper models may require more training data and careful regularization to avoid overfitting.\n",
    "    - Dropout Rate: Tuning the dropout rate (e.g., 0.2, 0.3, 0.5) can help prevent overfitting.\n",
    "    - Batch Size and Epochs: Increasing the batch size (e.g., 32, 64) and adjusting the number of epochs can affect both training speed and model performance. More epochs allow the model to learn better but may lead to overfitting if the model is trained for too long.\n",
    "    - Learning Rate: Adjusting the learning rate of the optimizer (e.g., adam) can affect convergence. You can try values like 0.001, 0.0001, or use learning rate scheduling to adjust the learning rate as training progresses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddc0878-a21b-40af-8403-10a378992d3e",
   "metadata": {},
   "source": [
    "##### Using learning rate schedulers\n",
    "A learning rate scheduler reduces the learning rate during training if the model reaches a plateau, which helps the model converge more efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54037ba-ba95-4482-a191-848d340bb61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.00001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82decd1-08aa-4773-8b5b-b57f9869f728",
   "metadata": {},
   "source": [
    "##### Using Bidirectional LSTMs\n",
    "Bidirectional LSTMs are capable of learning from both past and future data by processing the sequence in both directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395532bd-4c60-4a5c-adea-e4efab1a9f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional\n",
    "\n",
    "model.add(Bidirectional(LSTM(units=50, activation='relu', return_sequences=True)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff43db67-53ba-4171-a5a7-0203eb68d433",
   "metadata": {},
   "source": [
    "##### Using Early Stopping\n",
    "Early stopping stops the training process when the model's performance on the validation set stops improving, preventing overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35832f8f-aab6-4820-b0c7-a2899510e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621f8b6e-f419-48b7-9763-5614da27e79f",
   "metadata": {},
   "source": [
    "##### Tune the Window Size (Sequence Length)\n",
    "The window size (sequence length) has a direct impact on the performance of the LSTM. You can experiment with different sequence lengths (e.g., 30 days, 60 days) to see which one captures the temporal dependencies best\n",
    "\n",
    "Hereâ€™s is our approach for an optimized LSTM model with dropout, learning rate scheduling, and early stopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3118f6-bb4a-4f95-aa21-0b1a3b54e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Build the optimized LSTM model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(units=100, activation='relu', return_sequences=True, input_shape=(sequence_length, X_train_lstm.shape[2]))))\n",
    "model.add(Dropout(0.3))  # Increased dropout rate to prevent overfitting\n",
    "model.add(Bidirectional(LSTM(units=50, activation='relu', return_sequences=False)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=1, activation='sigmoid'))  # Output layer for binary classification\n",
    "\n",
    "# Compile the model with reduced learning rate\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Setup callbacks for early stopping and learning rate reduction\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.00001)\n",
    "\n",
    "# Train the model with early stopping and learning rate scheduler\n",
    "history = model.fit(X_train_lstm, y_train_lstm, epochs=50, batch_size=64, validation_data=(X_test_lstm, y_test_lstm),\n",
    "                    callbacks=[early_stopping, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe0e86b-1ac6-4a7a-9308-d984650df800",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAM_Bootcamp",
   "language": "python",
   "name": "cam_bootcamp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
